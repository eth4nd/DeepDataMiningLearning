{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')  # CUDA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps') #Apple GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries and modules\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Add the path to the directory where backbone.py is located\n",
    "sys.path.append('/data/cmpe258-sp24/013978029/DeepDataMiningLearning/hw1')  # Add the folder containing backbone.py\n",
    "\n",
    "# Import the backbone functions and classes\n",
    "from backbone import get_efficientnet_backbone, CustomBackboneWithFPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = get_efficientnet_backbone(pretrained=True)\n",
    "\n",
    "# Print all nested layers with full details to find exact layer names\n",
    "def print_all_layer_names_with_details(model, parent_name=\"\"):\n",
    "    for name, module in model.named_children():\n",
    "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        print(full_name)\n",
    "        print_all_layer_names_with_details(module, full_name)\n",
    "\n",
    "print(\"Detailed layer structure of EfficientNet-B0:\")\n",
    "print_all_layer_names_with_details(backbone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if specified layers exist in the model\n",
    "def check_layers_exist(model, layer_names):\n",
    "    existing_layers = set(name for name, _ in model.named_modules())\n",
    "    missing_layers = [layer for layer in layer_names if layer not in existing_layers]\n",
    "    \n",
    "    if missing_layers:\n",
    "        print(\"The following layers specified in return_layers do not exist in the model:\")\n",
    "        for layer in missing_layers:\n",
    "            print(f\" - {layer}\")\n",
    "    else:\n",
    "        print(\"All specified layers in return_layers exist in the model.\")\n",
    "\n",
    "# Initialize the EfficientNet-B0 backbone with pretrained weights\n",
    "backbone = get_efficientnet_backbone(pretrained=True)\n",
    "\n",
    "# Specify the layers to check\n",
    "return_layers = {\n",
    "            \"0.1\": \"0\",   # Early stage feature extraction\n",
    "            \"0.2\": \"1\",   # Mid-level feature extraction\n",
    "            \"0.4\": \"2\",   # Deeper stage feature extraction\n",
    "            \"0.6\": \"3\"    # Deepest stage before the final layer\n",
    "}\n",
    "\n",
    "# Check if the layers in return_layers exist in the backbone model\n",
    "check_layers_exist(backbone, return_layers.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize the model\n",
    "# Get the EfficientNet-B0 backbone with pretrained weights\n",
    "backbone = get_efficientnet_backbone(pretrained=True)\n",
    "\n",
    "# Specify the layer names to hook based on your model structure\n",
    "layer_names = [\n",
    "    \"0.1.0.block\",\n",
    "    \"0.2.1.block\",\n",
    "    \"0.4.2.block\",\n",
    "    \"0.6.3.block\"\n",
    "]\n",
    "\n",
    "# Create an instance of CustomBackboneWithFPN (no trainable_layers argument)\n",
    "model = CustomBackboneWithFPN(backbone, layer_names=layer_names, out_channels=256)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a test input tensor\n",
    "x = torch.rand(1, 3, 256, 256).to(device)  # Adjust input size as needed\n",
    "\n",
    "# Step 4: Run a forward pass and print the output shapes\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation for testing\n",
    "    output = model(x)\n",
    "\n",
    "# Print output shapes\n",
    "print(\"Output feature maps:\")\n",
    "for name, feature_map in output.items():\n",
    "    print(f\"Layer {name}: {feature_map.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"/data/cmpe258-sp24/013978029/dataset/Kitti\"\n",
    "if os.path.exists(path):\n",
    "    print(\"Path exists!\")\n",
    "else:\n",
    "    print(\"Path does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Paths to the directories containing the training and testing images\n",
    "train_image_dir = '/data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2'\n",
    "test_image_dir = '/data/cmpe258-sp24/013978029/dataset/Kitti/testing/image_2/'\n",
    "\n",
    "# Generate the list of image IDs (filenames without extensions) for training\n",
    "train_image_ids = [f.split('.')[0] for f in os.listdir(train_image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Generate the list of image IDs (filenames without extensions) for validation\n",
    "val_image_ids = [f.split('.')[0] for f in os.listdir(test_image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Create the ImageSets directory if it doesn't exist\n",
    "os.makedirs('/data/cmpe258-sp24/013978029/dataset/Kitti/ImageSets', exist_ok=True)\n",
    "\n",
    "# Write to train.txt\n",
    "with open('/data/cmpe258-sp24/013978029/dataset/Kitti/ImageSets/train.txt', 'w') as train_file:\n",
    "    for image_id in sorted(train_image_ids):\n",
    "        train_file.write(f\"{image_id}\\n\")\n",
    "\n",
    "# Write to val.txt\n",
    "with open('/data/cmpe258-sp24/013978029/dataset/Kitti/ImageSets/val.txt', 'w') as val_file:\n",
    "    for image_id in sorted(val_image_ids):\n",
    "        val_file.write(f\"{image_id}\\n\")\n",
    "\n",
    "print(\"train.txt and val.txt have been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/cmpe258-sp24/013978029/DeepDataMiningLearning/DeepDataMiningLearning/detection')  # Adjust this to your correct path\n",
    "\n",
    "# Import necessary functions and classes\n",
    "from dataset import get_dataset\n",
    "\n",
    "# Simulate argparse arguments\n",
    "class Args:\n",
    "    data_path = '/data/cmpe258-sp24/013978029/dataset/Kitti'  # Replace with your KITTI dataset path\n",
    "    annotationfile = ''\n",
    "    data_augmentation = 'hflip'\n",
    "    backend = 'PIL'\n",
    "    use_v2 = False\n",
    "    weights = None\n",
    "    test_only = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Load and print a sample\n",
    "dataset, _ = get_dataset('kitti', is_train=True, is_val=False, args=args)\n",
    "sample_image, sample_target = dataset[0]\n",
    "\n",
    "print(\"Sample Image Shape:\", sample_image.size)\n",
    "print(\"Sample Target:\", sample_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/cmpe258-sp24/013978029/DeepDataMiningLearning/DeepDataMiningLearning/detection')\n",
    "\n",
    "from dataset_kitti import MyKittiDetection, get_transformsimple\n",
    "\n",
    "# Create the dataset instance\n",
    "dataset = MyKittiDetection(root='/data/cmpe258-sp24/013978029/dataset/Kitti', train=True, transform=get_transformsimple(train=True))\n",
    "\n",
    "# Iterate over a few examples to check the output\n",
    "for i in range(3):  # Adjust the range as needed to test more images\n",
    "    img, target = dataset[i]\n",
    "    # The debug statements in `get_transformsimple` will print the image sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/cmpe258-sp24/013978029/DeepDataMiningLearning')  # Adjust this to your correct path\n",
    "\n",
    "# Import necessary functions and classes\n",
    "import torch\n",
    "from DeepDataMiningLearning.detection.dataset import get_dataset\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Simulate argparse arguments\n",
    "class Args:\n",
    "    data_path = '/data/cmpe258-sp24/013978029/dataset/Kitti'  # Replace with your KITTI dataset path\n",
    "    annotationfile = ''\n",
    "    data_augmentation = 'hflip'\n",
    "    backend = 'PIL'\n",
    "    use_v2 = False\n",
    "    weights = None\n",
    "    test_only = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Load the dataset\n",
    "dataset, _ = get_dataset('kitti', is_train=True, is_val=False, args=args)\n",
    "\n",
    "# Iterate through the dataset and print the size of each image\n",
    "for idx in range(len(dataset)):\n",
    "    image, target = dataset[idx]\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = F.to_pil_image(image)  # Convert Tensor to PIL Image for getting the size\n",
    "    print(f\"Image {idx + 1} Size: {image.size}\")  # Print (width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/cmpe258-sp24/013978029/DeepDataMiningLearning')  # Ensure the correct path is set\n",
    "\n",
    "# Import the get_dataset function from your dataset module\n",
    "from DeepDataMiningLearning.detection.dataset import get_dataset\n",
    "\n",
    "# Simulate the argparse arguments if needed\n",
    "class Args:\n",
    "    data_path = '/data/cmpe258-sp24/013978029/dataset/Kitti'  # Your dataset path\n",
    "    annotationfile = ''\n",
    "    dataset = 'kitti'\n",
    "    data_augmentation = 'hflip'\n",
    "    backend = 'PIL'\n",
    "    use_v2 = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Load the dataset with transformations applied\n",
    "dataset, _ = get_dataset('kitti', is_train=True, is_val=False, args=args)\n",
    "\n",
    "# Iterate through the dataset and print each image's transformed size\n",
    "print(\"Verifying transformed image sizes...\")\n",
    "consistent = True\n",
    "target_size = (3, 375, 1242)  # Replace with your target size if different\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    image, _ = dataset[idx]\n",
    "    print(f\"Image {idx + 1} Transformed Size: {image.shape}\")\n",
    "    \n",
    "    if image.shape != target_size:\n",
    "        print(f\"Inconsistent image size found: {image.shape} at index {idx}\")\n",
    "        consistent = False\n",
    "\n",
    "if consistent:\n",
    "    print(\"All images have consistent sizes.\")\n",
    "else:\n",
    "    print(\"Some images have inconsistent sizes. Check the output for details.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('/data/cmpe258-sp24/013978029/DeepDataMiningLearning') \n",
    "\n",
    "from DeepDataMiningLearning.detection.transforms import Resize, ToDtype, Compose\n",
    "from DeepDataMiningLearning.detection.dataset_kitti import KittiDataset\n",
    "\n",
    "# Remove PILToTensor as Resize now returns a tensor directly\n",
    "transform = Compose([Resize((370, 1224)), ToDtype(torch.float, scale=True)])\n",
    "dataset = KittiDataset(root=\"/data/cmpe258-sp24/013978029/dataset/Kitti\", train=True, transform=transform)\n",
    "\n",
    "for i in range(3):\n",
    "    img, target = dataset[i]\n",
    "    print(f\"[TEST] Image Shape: {img.shape}\")\n",
    "    print(f\"[TEST] Target Keys: {target.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define your dataset path\n",
    "data_path = '/data/cmpe258-sp24/013978029/dataset/Kitti'\n",
    "image_dir = os.path.join(data_path, 'raw/training/image_2')\n",
    "label_dir = os.path.join(data_path, 'raw/training/label_2')\n",
    "\n",
    "# Check if the required directories exist\n",
    "def check_directory_structure():\n",
    "    if not os.path.exists(image_dir):\n",
    "        print(f\"Image directory missing: {image_dir}\")\n",
    "    else:\n",
    "        print(f\"Image directory exists: {image_dir}\")\n",
    "    \n",
    "    if not os.path.exists(label_dir):\n",
    "        print(f\"Label directory missing: {label_dir}\")\n",
    "    else:\n",
    "        print(f\"Label directory exists: {label_dir}\")\n",
    "\n",
    "# Check if the first few files exist\n",
    "def check_sample_files(num_samples=20):\n",
    "    missing_images = []\n",
    "    missing_labels = []\n",
    "    for i in range(num_samples):\n",
    "        image_path = os.path.join(image_dir, f\"{i:06d}.png\")\n",
    "        label_path = os.path.join(label_dir, f\"{i:06d}.txt\")\n",
    "        \n",
    "        if not os.path.isfile(image_path):\n",
    "            missing_images.append(image_path)\n",
    "        \n",
    "        if not os.path.isfile(label_path):\n",
    "            missing_labels.append(label_path)\n",
    "    \n",
    "    if missing_images:\n",
    "        print(f\"Missing {len(missing_images)} image files:\")\n",
    "        for path in missing_images:\n",
    "            print(f\" - {path}\")\n",
    "    else:\n",
    "        print(\"All expected image files are present.\")\n",
    "\n",
    "    if missing_labels:\n",
    "        print(f\"Missing {len(missing_labels)} label files:\")\n",
    "        for path in missing_labels:\n",
    "            print(f\" - {path}\")\n",
    "    else:\n",
    "        print(\"All expected label files are present.\")\n",
    "\n",
    "# Run the checks\n",
    "print(\"Checking dataset directory structure...\")\n",
    "check_directory_structure()\n",
    "\n",
    "print(\"\\nChecking sample image and label files...\")\n",
    "check_sample_files(num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the directory we want to verify\n",
    "image_dir = '/data/cmpe258-sp24/013978029/dataset/Kitti/raw/testing/image_2'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.isdir(image_dir):\n",
    "    print(f\"Directory exists: {image_dir}\")\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(image_dir)\n",
    "    \n",
    "    # Filter to only include .png files\n",
    "    png_files = [f for f in files if f.endswith('.png')]\n",
    "    \n",
    "    if png_files:\n",
    "        # Sort files to check the sequence\n",
    "        png_files.sort()\n",
    "\n",
    "        # Check the naming format and sequence\n",
    "        all_sequential = True\n",
    "        for i, file_name in enumerate(png_files):\n",
    "            expected_name = f\"{i:06}.png\"\n",
    "            if file_name != expected_name:\n",
    "                print(f\"File {file_name} does not match expected name {expected_name}.\")\n",
    "                all_sequential = False\n",
    "                break\n",
    "        \n",
    "        if all_sequential:\n",
    "            print(\"All images are present and follow the expected naming format.\")\n",
    "        else:\n",
    "            print(\"Some images are missing or incorrectly named.\")\n",
    "    else:\n",
    "        print(\"No PNG image files found in the directory.\")\n",
    "else:\n",
    "    print(f\"Directory does not exist: {image_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/013978029/.conda/envs/cmpe249project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x7f4acd514b30>\n",
      "Loading data\n",
      "train set len: 4064\n",
      "test set len: 4065\n",
      "Creating data loaders\n",
      "Creating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/013978029/.conda/envs/cmpe249project/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/001195.pngChecking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/000245.png\n",
      "\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/001119.png\n",
      "[DEBUG] get_label for 000245 returns 5 objects.\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/002554.png\n",
      "[DEBUG] Converting target for image_id 000245: [{'image_id': '000245', 'type': 'Car', 'truncated': 0.39, 'occluded': 1, 'alpha': 2.49, 'bbox': [871.24, 138.44, 1241.0, 369.5], 'dimensions': [1.85, 1.68, 4.34], 'location': [4.89, 1.58, 6.76], 'rotation_y': 3.1}, {'image_id': '000245', 'type': 'Cyclist', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.59, 'bbox': [593.67, 172.0, 612.03, 219.1], 'dimensions': [1.71, 0.63, 2.12], 'location': [-0.32, 1.7, 27.39], 'rotation_y': -1.6}, {'image_id': '000245', 'type': 'Car', 'truncated': 0.48, 'occluded': 0, 'alpha': -2.51, 'bbox': [0.0, 199.5, 230.56, 336.43], 'dimensions': [1.43, 1.56, 3.85], 'location': [-6.94, 1.8, 8.64], 'rotation_y': 3.12}, {'image_id': '000245', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -2.81, 'bbox': [174.45, 190.89, 368.89, 263.55], 'dimensions': [1.42, 1.62, 3.5], 'location': [-7.3, 1.85, 15.76], 'rotation_y': 3.04}, {'image_id': '000245', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -2.94, 'bbox': [363.25, 182.66, 466.82, 221.54], 'dimensions': [1.41, 1.54, 3.6], 'location': [-7.48, 1.81, 27.77], 'rotation_y': 3.08}][DEBUG] get_label for 001195 returns 10 objects.\n",
      "[DEBUG] Converting target for image_id 001195: [{'image_id': '001195', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.57, 'bbox': [559.84, 178.18, 617.21, 230.49], 'dimensions': [1.6, 1.76, 3.84], 'location': [-0.68, 1.38, 24.15], 'rotation_y': -1.6}, {'image_id': '001195', 'type': 'Car', 'truncated': 0.46, 'occluded': 0, 'alpha': -0.92, 'bbox': [0.0, 184.94, 195.43, 326.92], 'dimensions': [1.55, 1.55, 3.94], 'location': [-7.58, 1.57, 9.78], 'rotation_y': -1.57}, {'image_id': '001195', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.23, 'bbox': [191.63, 178.22, 453.98, 349.05], 'dimensions': [1.65, 1.7, 3.9], 'location': [-3.23, 1.57, 8.93], 'rotation_y': -1.57}, {'image_id': '001195', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.22, 'bbox': [288.92, 177.86, 392.86, 239.47], 'dimensions': [1.66, 1.62, 3.95], 'location': [-7.87, 1.45, 21.5], 'rotation_y': -1.57}, {'image_id': '001195', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.43, 'bbox': [465.38, 177.79, 539.0, 234.98], 'dimensions': [1.64, 1.72, 4.24], 'location': [-3.3, 1.42, 22.96], 'rotation_y': -1.57}, {'image_id': '001195', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.35, 'bbox': [407.78, 182.23, 459.64, 214.74], 'dimensions': [1.45, 1.61, 3.72], 'location': [-8.29, 1.32, 34.42], 'rotation_y': -1.58}, {'image_id': '001195', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.17, 'bbox': [255.45, 183.62, 335.37, 222.61], 'dimensions': [1.42, 1.51, 3.84], 'location': [-12.42, 1.38, 28.76], 'rotation_y': -1.57}, {'image_id': '001195', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.46, 'bbox': [503.36, 182.23, 545.43, 213.98], 'dimensions': [1.48, 1.64, 3.81], 'location': [-4.13, 1.34, 35.87], 'rotation_y': -1.57}, {'image_id': '001195', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.49, 'bbox': [528.18, 181.92, 557.21, 203.39], 'dimensions': [1.37, 1.61, 4.21], 'location': [-4.39, 1.16, 48.82], 'rotation_y': -1.58}, {'image_id': '001195', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.03, 'bbox': [53.17, 187.46, 200.65, 250.8], 'dimensions': [1.42, 1.56, 3.39], 'location': [-12.15, 1.5, 18.37], 'rotation_y': -1.61}]\n",
      "\n",
      "[DEBUG] Original image size (PIL): (1241, 376)\n",
      "[DEBUG] Applying transform: Resize()\n",
      "[DEBUG] Original Image Size: (1241, 376)\n",
      "[DEBUG] Original image size (PIL): (1242, 375)[DEBUG] get_label for 001119 returns 5 objects.\n",
      "\n",
      "[DEBUG] Applying transform: Resize()\n",
      "[DEBUG] Converting target for image_id 001119: [{'image_id': '001119', 'type': 'Car', 'truncated': 1.0, 'occluded': 0, 'alpha': 2.5, 'bbox': [0.0, 209.77, 127.87, 374.0], 'dimensions': [1.62, 1.75, 4.3], 'location': [-4.15, 1.85, 2.7], 'rotation_y': 1.55}, {'image_id': '001119', 'type': 'Car', 'truncated': 0.93, 'occluded': 3, 'alpha': 2.42, 'bbox': [0.0, 205.74, 100.41, 349.6], 'dimensions': [1.47, 1.73, 4.48], 'location': [-7.42, 1.88, 6.87], 'rotation_y': 1.61}, {'image_id': '001119', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.13, 'bbox': [247.64, 202.13, 366.05, 265.63], 'dimensions': [1.39, 1.59, 3.17], 'location': [-7.74, 2.2, 18.61], 'rotation_y': -1.52}, {'image_id': '001119', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.01, 'bbox': [152.18, 201.88, 304.6, 268.48], 'dimensions': [1.46, 1.55, 4.28], 'location': [-10.1, 2.31, 19.41], 'rotation_y': -1.49}, {'image_id': '001119', 'type': 'Car', 'truncated': 0.63, 'occluded': 3, 'alpha': -0.79, 'bbox': [0.0, 208.37, 81.79, 267.84], 'dimensions': [1.49, 1.66, 4.16], 'location': [-17.23, 2.57, 20.1], 'rotation_y': -1.49}][DEBUG] Original Image Size: (1242, 375)\n",
      "\n",
      "[DEBUG] Original image size (PIL): (1242, 375)[DEBUG] get_label for 002554 returns 1 objects.\n",
      "[DEBUG] Applying transform: Resize()[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Converting target for image_id 002554: [{'image_id': '002554', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 0, 'alpha': 2.86, 'bbox': [801.86, 153.14, 842.86, 288.14], 'dimensions': [1.75, 0.43, 0.89], 'location': [2.89, 1.4, 9.45], 'rotation_y': -3.14}]\n",
      "\n",
      "[DEBUG] Original Image Size: (1242, 375)[DEBUG] Original image size (PIL): (1224, 370)[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: PILToTensor()[DEBUG] Applying transform: Resize()[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Original Image Size: (1224, 370)[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "\n",
      "\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] Applying transform: PILToTensor()[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/000903.png\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "\n",
      "[DEBUG] Applying transform: ToDtype()[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/003876.png\n",
      "\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] get_label for 000903 returns 6 objects.\n",
      "\n",
      "[DEBUG] Converting target for image_id 000903: [{'image_id': '000903', 'type': 'Car', 'truncated': 0.45, 'occluded': 1, 'alpha': 1.86, 'bbox': [1096.8, 175.74, 1241.0, 315.82], 'dimensions': [1.46, 1.49, 3.63], 'location': [7.73, 1.5, 9.05], 'rotation_y': 2.55}, {'image_id': '000903', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 2.63, 'bbox': [857.43, 152.99, 1135.39, 266.49], 'dimensions': [1.71, 1.67, 3.7], 'location': [6.22, 1.42, 11.96], 'rotation_y': 3.1}, {'image_id': '000903', 'type': 'Van', 'truncated': 0.0, 'occluded': 2, 'alpha': 2.66, 'bbox': [806.19, 151.15, 1105.63, 252.64], 'dimensions': [1.86, 1.85, 5.18], 'location': [6.79, 1.48, 14.58], 'rotation_y': 3.09}, {'image_id': '000903', 'type': 'Car', 'truncated': 0.48, 'occluded': 1, 'alpha': 2.0, 'bbox': [0.0, 188.97, 323.66, 374.0], 'dimensions': [1.64, 1.63, 4.51], 'location': [-4.39, 1.83, 7.15], 'rotation_y': 1.47}, {'image_id': '000903', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.86, 'bbox': [241.62, 185.27, 416.01, 296.8], 'dimensions': [1.6, 1.66, 4.33], 'location': [-4.78, 1.84, 12.84], 'rotation_y': 1.5}, {'image_id': '000903', 'type': 'Truck', 'truncated': 0.0, 'occluded': 3, 'alpha': -1.21, 'bbox': [532.78, 121.53, 691.68, 224.42], 'dimensions': [3.52, 2.89, 10.81], 'location': [0.56, 1.81, 30.22], 'rotation_y': -1.2}][DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] Original image size (PIL): (1242, 375)\n",
      "[DEBUG] Applying transform: Resize()\n",
      "[DEBUG] Original Image Size: (1242, 375)\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/002631.png[DEBUG] get_label for 003876 returns 11 objects.[DEBUG] Applying transform: ToDtype()\n",
      "\n",
      "[DEBUG] Converting target for image_id 003876: [{'image_id': '003876', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.57, 'bbox': [562.43, 175.49, 677.92, 284.63], 'dimensions': [1.56, 1.65, 3.69], 'location': [0.13, 1.62, 12.25], 'rotation_y': -1.56}, {'image_id': '003876', 'type': 'Car', 'truncated': 0.92, 'occluded': 0, 'alpha': -1.48, 'bbox': [1093.73, 162.62, 1241.0, 374.0], 'dimensions': [1.54, 1.73, 4.04], 'location': [4.31, 1.49, 3.93], 'rotation_y': -0.68}, {'image_id': '003876', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.28, 'bbox': [898.21, 177.28, 1181.2, 352.34], 'dimensions': [1.38, 1.56, 3.8], 'location': [4.64, 1.44, 7.62], 'rotation_y': -0.75}, {'image_id': '003876', 'type': 'Van', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.7, 'bbox': [494.38, 171.2, 534.53, 202.74], 'dimensions': [2.02, 1.99, 5.56], 'location': [-6.49, 1.95, 49.47], 'rotation_y': 1.57}, {'image_id': '003876', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 0, 'alpha': -0.89, 'bbox': [796.66, 165.56, 822.66, 223.34], 'dimensions': [1.64, 0.63, 0.86], 'location': [5.69, 1.44, 20.98], 'rotation_y': -0.63}, {'image_id': '003876', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 2, 'alpha': 1.95, 'bbox': [933.52, 169.98, 950.19, 211.37], 'dimensions': [1.49, 0.55, 0.78], 'location': [12.17, 1.39, 26.44], 'rotation_y': 2.37}, {'image_id': '003876', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 2, 'alpha': 2.59, 'bbox': [1036.53, 161.11, 1051.53, 215.94], 'dimensions': [1.81, 0.51, 0.34], 'location': [14.42, 1.42, 24.05], 'rotation_y': 3.12}, {'image_id': '003876', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 2, 'alpha': 2.22, 'bbox': [997.44, 167.75, 1006.44, 204.51], 'dimensions': [1.8, 0.43, 0.2], 'location': [19.22, 1.55, 35.57], 'rotation_y': 2.71}, {'image_id': '003876', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 0.51, 'bbox': [52.49, 189.17, 148.14, 220.89], 'dimensions': [1.43, 1.45, 3.5], 'location': [-24.53, 2.25, 34.75], 'rotation_y': -0.1}, {'image_id': '003876', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [456.52, 179.24, 485.92, 197.65], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '003876', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [545.79, 175.36, 556.43, 184.06], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "\n",
      "[DEBUG] Original image size (PIL): (1242, 375)\n",
      "[DEBUG] get_label for 002631 returns 8 objects.\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Applying transform: Resize()\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "[DEBUG] Converting target for image_id 002631: [{'image_id': '002631', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.39, 'bbox': [1077.91, 178.2, 1204.52, 261.88], 'dimensions': [1.65, 1.67, 3.64], 'location': [11.99, 1.79, 16.25], 'rotation_y': -0.76}, {'image_id': '002631', 'type': 'Van', 'truncated': 0.88, 'occluded': 1, 'alpha': -3.12, 'bbox': [0.0, 151.09, 107.16, 330.84], 'dimensions': [2.01, 1.66, 4.35], 'location': [-8.3, 1.78, 7.84], 'rotation_y': 2.37}, {'image_id': '002631', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 2.33, 'bbox': [565.21, 180.46, 735.59, 261.65], 'dimensions': [1.59, 1.55, 3.93], 'location': [0.96, 1.79, 16.34], 'rotation_y': 2.39}, {'image_id': '002631', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 2.02, 'bbox': [838.4, 181.67, 940.9, 229.84], 'dimensions': [1.47, 1.57, 4.24], 'location': [9.56, 1.79, 24.59], 'rotation_y': 2.38}, {'image_id': '002631', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.88, 'bbox': [970.6, 182.83, 1045.52, 217.01], 'dimensions': [1.4, 1.68, 4.45], 'location': [18.02, 1.88, 32.63], 'rotation_y': 2.38}, {'image_id': '002631', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 1.82, 'bbox': [1039.38, 182.2, 1096.98, 212.49], 'dimensions': [1.45, 1.62, 3.77], 'location': [23.71, 1.96, 37.33], 'rotation_y': 2.38}, {'image_id': '002631', 'type': 'Car', 'truncated': 0.72, 'occluded': 0, 'alpha': -1.48, 'bbox': [1222.82, 183.36, 1241.0, 222.28], 'dimensions': [1.5, 1.61, 4.07], 'location': [26.61, 1.95, 29.77], 'rotation_y': -0.76}, {'image_id': '002631', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [108.5, 277.56, 168.31, 308.26], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Original image size (PIL): (1242, 375)\n",
      "\n",
      "[DEBUG] Applying transform: Resize()\n",
      "[DEBUG] Original Image Size: (1242, 375)[DEBUG] Applying transform: PILToTensor()Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/003797.png\n",
      "\n",
      "[DEBUG] Original Image Size: (1242, 375)\n",
      "\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] get_label for 003797 returns 1 objects.[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "\n",
      "\n",
      "[DEBUG] Converting target for image_id 003797: [{'image_id': '003797', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 0, 'alpha': 2.75, 'bbox': [871.68, 153.06, 933.94, 287.99], 'dimensions': [1.75, 0.43, 0.89], 'location': [3.96, 1.4, 9.46], 'rotation_y': -3.14}][DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Original image size (PIL): (1224, 370)\n",
      "[DEBUG] Applying transform: Resize()\n",
      "\n",
      "[DEBUG] Applying transform: ToDtype()[DEBUG] Original Image Size: (1224, 370)\n",
      "\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/002892.png\n",
      "\n",
      "[DEBUG] Applying transform: ToDtype()[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "\n",
      "[DEBUG] Applying transform: ToDtype()[DEBUG] get_label for 002892 returns 13 objects.\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Converting target for image_id 002892: [{'image_id': '002892', 'type': 'Pedestrian', 'truncated': 0.31, 'occluded': 0, 'alpha': -2.11, 'bbox': [891.04, 147.45, 1115.72, 373.0], 'dimensions': [1.76, 0.76, 1.02], 'location': [2.36, 1.58, 4.42], 'rotation_y': -1.65}, {'image_id': '002892', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.78, 'bbox': [415.99, 157.81, 458.24, 264.07], 'dimensions': [1.89, 0.96, 0.95], 'location': [-3.24, 1.46, 13.38], 'rotation_y': -2.02}, {'image_id': '002892', 'type': 'Pedestrian', 'truncated': 0.06, 'occluded': 1, 'alpha': -2.09, 'bbox': [894.66, 163.36, 970.98, 373.0], 'dimensions': [1.68, 0.64, 0.91], 'location': [2.65, 1.55, 5.91], 'rotation_y': -1.69}, {'image_id': '002892', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 0, 'alpha': -2.23, 'bbox': [744.53, 161.1, 804.15, 348.53], 'dimensions': [1.71, 0.68, 0.93], 'location': [1.6, 1.52, 7.1], 'rotation_y': -2.02}, {'image_id': '002892', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 0, 'alpha': -2.35, 'bbox': [557.06, 163.86, 624.19, 315.86], 'dimensions': [1.71, 0.8, 0.94], 'location': [-0.25, 1.52, 8.71], 'rotation_y': -2.38}, {'image_id': '002892', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 2, 'alpha': -2.54, 'bbox': [764.49, 155.94, 863.52, 303.56], 'dimensions': [1.8, 0.83, 0.92], 'location': [2.73, 1.49, 9.37], 'rotation_y': -2.27}, {'image_id': '002892', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 0, 'alpha': -2.81, 'bbox': [660.59, 159.8, 708.21, 280.19], 'dimensions': [1.82, 0.68, 0.99], 'location': [1.18, 1.5, 11.39], 'rotation_y': -2.72}, {'image_id': '002892', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 1, 'alpha': -2.47, 'bbox': [749.02, 173.28, 820.73, 334.07], 'dimensions': [1.63, 0.71, 0.88], 'location': [2.11, 1.55, 7.84], 'rotation_y': -2.23}, {'image_id': '002892', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 1, 'alpha': -2.51, 'bbox': [708.21, 157.14, 754.4, 269.11], 'dimensions': [1.83, 0.78, 0.94], 'location': [2.06, 1.43, 12.35], 'rotation_y': -2.35}, {'image_id': '002892', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 2, 'alpha': -2.18, 'bbox': [720.58, 161.06, 804.81, 298.85], 'dimensions': [1.72, 0.69, 0.88], 'location': [2.08, 1.46, 9.51], 'rotation_y': -1.98}, {'image_id': '002892', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.72, 'bbox': [489.03, 171.61, 571.72, 234.41], 'dimensions': [1.57, 1.67, 4.14], 'location': [-1.9, 1.32, 20.18], 'rotation_y': 1.63}, {'image_id': '002892', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [626.56, 163.29, 661.02, 217.74], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '002892', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [356.39, 178.76, 377.96, 213.87], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/003997.png[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Original image size (PIL): (1238, 374)\n",
      "\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "\n",
      "[DEBUG] Applying transform: Resize()[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/001649.png\n",
      "\n",
      "[DEBUG] Original Image Size: (1238, 374)Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/000432.png\n",
      "\n",
      "[DEBUG] get_label for 000432 returns 16 objects.[DEBUG] get_label for 003997 returns 3 objects.\n",
      "\n",
      "[DEBUG] Converting target for image_id 003997: [{'image_id': '003997', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 0, 'alpha': -0.4, 'bbox': [812.61, 142.57, 887.64, 299.58], 'dimensions': [1.85, 0.67, 1.1], 'location': [3.01, 1.4, 8.74], 'rotation_y': -0.07}, {'image_id': '003997', 'type': 'Pedestrian', 'truncated': 0.33, 'occluded': 0, 'alpha': 1.85, 'bbox': [0.0, 152.54, 88.65, 336.18], 'dimensions': [1.74, 0.77, 0.87], 'location': [-5.86, 1.48, 7.13], 'rotation_y': 1.18}, {'image_id': '003997', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [0.0, 159.0, 93.99, 349.25], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}][DEBUG] Converting target for image_id 000432: [{'image_id': '000432', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.58, 'bbox': [586.96, 172.97, 661.42, 243.34], 'dimensions': [1.56, 1.65, 3.69], 'location': [0.3, 1.59, 17.99], 'rotation_y': -1.56}, {'image_id': '000432', 'type': 'Car', 'truncated': 0.57, 'occluded': 0, 'alpha': -1.44, 'bbox': [1037.72, 151.37, 1241.0, 374.0], 'dimensions': [1.61, 1.67, 3.65], 'location': [4.9, 1.48, 5.78], 'rotation_y': -0.77}, {'image_id': '000432', 'type': 'Van', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.14, 'bbox': [751.83, 141.27, 969.7, 272.06], 'dimensions': [1.88, 1.82, 4.64], 'location': [4.52, 1.44, 12.67], 'rotation_y': -0.8}, {'image_id': '000432', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.02, 'bbox': [699.25, 168.37, 831.18, 228.19], 'dimensions': [1.49, 1.63, 4.16], 'location': [4.35, 1.39, 20.01], 'rotation_y': -0.81}, {'image_id': '000432', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -0.86, 'bbox': [666.73, 168.57, 741.3, 199.84], 'dimensions': [1.54, 1.72, 3.88], 'location': [4.9, 1.34, 37.56], 'rotation_y': -0.73}, {'image_id': '000432', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -0.87, 'bbox': [655.05, 170.15, 705.78, 193.66], 'dimensions': [1.52, 1.49, 3.46], 'location': [4.73, 1.36, 48.48], 'rotation_y': -0.77}, {'image_id': '000432', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 2.1, 'bbox': [93.73, 175.5, 264.55, 250.0], 'dimensions': [1.67, 1.68, 4.49], 'location': [-10.87, 1.76, 18.56], 'rotation_y': 1.58}, {'image_id': '000432', 'type': 'Van', 'truncated': 0.0, 'occluded': 0, 'alpha': 1.93, 'bbox': [228.83, 119.52, 415.61, 253.54], 'dimensions': [2.91, 2.18, 6.91], 'location': [-7.26, 1.79, 19.23], 'rotation_y': 1.58}, {'image_id': '000432', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 2.0, 'bbox': [247.61, 178.22, 347.71, 229.61], 'dimensions': [1.63, 1.58, 4.2], 'location': [-10.78, 1.83, 25.14], 'rotation_y': 1.59}, {'image_id': '000432', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.91, 'bbox': [334.17, 181.96, 402.07, 214.96], 'dimensions': [1.34, 1.57, 4.01], 'location': [-10.63, 1.76, 31.87], 'rotation_y': 1.59}, {'image_id': '000432', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': 1.83, 'bbox': [434.55, 175.69, 477.08, 198.76], 'dimensions': [1.45, 1.65, 4.63], 'location': [-10.28, 1.67, 48.28], 'rotation_y': 1.62}, {'image_id': '000432', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 1.78, 'bbox': [459.35, 170.77, 491.17, 195.81], 'dimensions': [1.81, 1.59, 3.93], 'location': [-10.19, 1.69, 54.7], 'rotation_y': 1.59}, {'image_id': '000432', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.86, 'bbox': [489.15, 172.75, 520.27, 190.83], 'dimensions': [1.56, 1.62, 4.19], 'location': [-9.47, 1.58, 65.1], 'rotation_y': 1.72}, {'image_id': '000432', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 2.54, 'bbox': [1061.63, 180.61, 1205.24, 216.89], 'dimensions': [1.34, 1.47, 4.51], 'location': [20.4, 1.67, 28.27], 'rotation_y': -3.13}, {'image_id': '000432', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 2.09, 'bbox': [171.0, 180.07, 250.58, 221.98], 'dimensions': [1.47, 1.29, 2.95], 'location': [-14.86, 1.74, 26.92], 'rotation_y': 1.59}, {'image_id': '000432', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [528.07, 170.82, 553.93, 186.69], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}][DEBUG] get_label for 001649 returns 10 objects.\n",
      "\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] Original image size (PIL): (1224, 370)\n",
      "\n",
      "[DEBUG] Original image size (PIL): (1242, 375)\n",
      "[DEBUG] Applying transform: Resize()\n",
      "[DEBUG] Original Image Size: (1224, 370)\n",
      "[DEBUG] Converting target for image_id 001649: [{'image_id': '001649', 'type': 'Car', 'truncated': 0.87, 'occluded': 1, 'alpha': -2.3, 'bbox': [0.0, 151.72, 141.23, 374.0], 'dimensions': [1.74, 1.75, 4.57], 'location': [-6.52, 1.63, 5.56], 'rotation_y': -3.14}, {'image_id': '001649', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 0.57, 'bbox': [76.39, 164.06, 376.09, 280.6], 'dimensions': [1.72, 1.68, 4.02], 'location': [-6.13, 1.62, 11.68], 'rotation_y': 0.1}, {'image_id': '001649', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -2.76, 'bbox': [157.59, 179.45, 413.24, 259.34], 'dimensions': [1.41, 1.42, 4.27], 'location': [-6.16, 1.56, 13.89], 'rotation_y': 3.11}, {'image_id': '001649', 'type': 'Car', 'truncated': 0.44, 'occluded': 3, 'alpha': -2.49, 'bbox': [0.0, 180.21, 258.44, 321.2], 'dimensions': [1.5, 1.65, 4.14], 'location': [-6.73, 1.62, 8.63], 'rotation_y': -3.14}, {'image_id': '001649', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 0.21, 'bbox': [425.79, 174.16, 539.31, 209.5], 'dimensions': [1.38, 1.66, 4.38], 'location': [-5.27, 1.47, 29.95], 'rotation_y': 0.04}, {'image_id': '001649', 'type': 'Car', 'truncated': 0.32, 'occluded': 1, 'alpha': 0.88, 'bbox': [1085.94, 160.61, 1241.0, 242.45], 'dimensions': [1.69, 1.73, 4.3], 'location': [13.58, 1.45, 17.09], 'rotation_y': 1.54}, {'image_id': '001649', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -0.11, 'bbox': [678.59, 172.21, 804.42, 216.52], 'dimensions': [1.48, 1.86, 4.18], 'location': [4.54, 1.49, 25.63], 'rotation_y': 0.06}, {'image_id': '001649', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -0.15, 'bbox': [666.29, 166.13, 782.25, 211.96], 'dimensions': [1.74, 1.66, 4.37], 'location': [4.49, 1.51, 28.95], 'rotation_y': 0.0}, {'image_id': '001649', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [605.31, 168.22, 618.59, 197.42], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '001649', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [501.25, 176.25, 557.42, 198.58], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "[DEBUG] Original image size (PIL): (1242, 375)\n",
      "\n",
      "[DEBUG] Applying transform: Resize()[DEBUG] Applying transform: Resize()[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] Original Image Size: (1242, 375)\n",
      "[DEBUG] Original Image Size: (1242, 375)[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "\n",
      "[DEBUG] Applying transform: ToDtype()[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/001204.png\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/000725.png\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "\n",
      "[DEBUG] Applying transform: PILToTensor()[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] get_label for 000725 returns 9 objects.\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Converting target for image_id 000725: [{'image_id': '000725', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.57, 'bbox': [572.1, 179.41, 604.46, 208.92], 'dimensions': [1.6, 1.76, 3.84], 'location': [-1.14, 1.28, 41.33], 'rotation_y': -1.6}, {'image_id': '000725', 'type': 'Car', 'truncated': 0.99, 'occluded': 0, 'alpha': -0.68, 'bbox': [0.0, 214.88, 139.66, 375.0], 'dimensions': [1.38, 1.71, 4.49], 'location': [-4.76, 1.64, 3.64], 'rotation_y': -1.56}, {'image_id': '000725', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.04, 'bbox': [21.22, 186.63, 254.12, 293.05], 'dimensions': [1.54, 1.65, 3.82], 'location': [-7.99, 1.59, 12.64], 'rotation_y': -1.6}, {'image_id': '000725', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.16, 'bbox': [217.15, 186.04, 352.92, 252.83], 'dimensions': [1.49, 1.67, 4.05], 'location': [-8.19, 1.54, 18.59], 'rotation_y': -1.57}, {'image_id': '000725', 'type': 'Van', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.32, 'bbox': [306.26, 119.16, 477.84, 271.03], 'dimensions': [2.74, 2.09, 5.96], 'location': [-4.49, 1.53, 16.05], 'rotation_y': -1.58}, {'image_id': '000725', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.43, 'bbox': [463.32, 184.91, 518.24, 224.49], 'dimensions': [1.41, 1.58, 3.99], 'location': [-4.57, 1.42, 28.39], 'rotation_y': -1.59}, {'image_id': '000725', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.44, 'bbox': [493.42, 184.16, 531.29, 211.84], 'dimensions': [1.42, 1.61, 3.56], 'location': [-5.21, 1.38, 39.45], 'rotation_y': -1.57}, {'image_id': '000725', 'type': 'Misc', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.52, 'bbox': [536.4, 174.13, 556.06, 196.14], 'dimensions': [2.23, 1.88, 2.54], 'location': [-6.36, 1.1, 74.65], 'rotation_y': -1.6}, {'image_id': '000725', 'type': 'Car', 'truncated': 0.21, 'occluded': 3, 'alpha': -0.95, 'bbox': [0.0, 184.54, 167.89, 270.75], 'dimensions': [1.59, 1.71, 3.74], 'location': [-11.42, 1.59, 15.35], 'rotation_y': -1.58}]\n",
      "[DEBUG] get_label for 001204 returns 9 objects.[DEBUG] Applying transform: ToDtype()[DEBUG] Original image size (PIL): (1241, 376)\n",
      "\n",
      "[DEBUG] Applying transform: ToDtype()[DEBUG] Applying transform: Resize()[DEBUG] Converting target for image_id 001204: [{'image_id': '001204', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 1.89, 'bbox': [559.01, 188.34, 695.98, 275.86], 'dimensions': [1.48, 1.51, 4.35], 'location': [0.49, 1.83, 14.98], 'rotation_y': 1.92}, {'image_id': '001204', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': 2.03, 'bbox': [560.68, 189.85, 777.89, 315.11], 'dimensions': [1.47, 1.68, 3.88], 'location': [1.03, 1.76, 10.93], 'rotation_y': 2.13}, {'image_id': '001204', 'type': 'Car', 'truncated': 0.96, 'occluded': 0, 'alpha': 2.68, 'bbox': [0.0, 203.72, 404.82, 374.0], 'dimensions': [1.46, 1.74, 3.99], 'location': [-3.02, 1.67, 3.31], 'rotation_y': 1.98}, {'image_id': '001204', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.82, 'bbox': [650.01, 186.88, 747.54, 245.92], 'dimensions': [1.42, 1.68, 4.29], 'location': [2.57, 1.85, 20.28], 'rotation_y': 1.95}, {'image_id': '001204', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 1.76, 'bbox': [722.23, 186.73, 779.46, 227.27], 'dimensions': [1.36, 1.4, 3.8], 'location': [5.3, 1.91, 27.01], 'rotation_y': 1.95}, {'image_id': '001204', 'type': 'Cyclist', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.52, 'bbox': [962.76, 170.19, 979.37, 207.1], 'dimensions': [1.69, 0.57, 2.0], 'location': [17.04, 1.58, 34.12], 'rotation_y': 1.98}, {'image_id': '001204', 'type': 'Van', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.6, 'bbox': [797.88, 164.42, 833.51, 204.88], 'dimensions': [2.52, 2.1, 5.64], 'location': [13.73, 2.03, 47.99], 'rotation_y': 1.87}, {'image_id': '001204', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': 1.61, 'bbox': [781.59, 182.11, 815.2, 209.76], 'dimensions': [1.44, 1.65, 2.96], 'location': [10.37, 1.97, 39.72], 'rotation_y': 1.87}, {'image_id': '001204', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [836.88, 163.71, 881.16, 197.64], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Original image size (PIL): (1242, 375)\n",
      "\n",
      "[DEBUG] Original Image Size: (1241, 376)\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/000704.png\n",
      "\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] Applying transform: Resize()[DEBUG] get_label for 000704 returns 4 objects.\n",
      "\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])[DEBUG] Converting target for image_id 000704: [{'image_id': '000704', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.88, 'bbox': [750.01, 163.51, 1068.4, 365.01], 'dimensions': [1.49, 1.56, 4.34], 'location': [2.61, 1.45, 7.55], 'rotation_y': -1.56}, {'image_id': '000704', 'type': 'Car', 'truncated': 0.99, 'occluded': 0, 'alpha': -0.8, 'bbox': [0.0, 207.75, 285.16, 374.0], 'dimensions': [1.42, 1.53, 4.12], 'location': [-2.99, 1.65, 2.78], 'rotation_y': -1.58}, {'image_id': '000704', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.62, 'bbox': [658.61, 173.96, 712.98, 218.01], 'dimensions': [1.38, 1.64, 3.51], 'location': [2.47, 1.44, 24.53], 'rotation_y': -1.52}, {'image_id': '000704', 'type': 'Van', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.3, 'bbox': [702.45, 152.41, 744.27, 186.13], 'dimensions': [2.59, 1.98, 5.33], 'location': [9.23, 1.04, 58.41], 'rotation_y': -1.15}]\n",
      "\n",
      "[DEBUG] Original Image Size: (1242, 375)Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/000306.png\n",
      "[DEBUG] Original image size (PIL): (1242, 375)\n",
      "[DEBUG] Applying transform: Resize()\n",
      "[DEBUG] Original Image Size: (1242, 375)\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] get_label for 000306 returns 7 objects.\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Converting target for image_id 000306: [{'image_id': '000306', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.57, 'bbox': [556.85, 173.12, 619.99, 236.08], 'dimensions': [1.62, 1.63, 4.5], 'location': [-0.67, 1.66, 21.05], 'rotation_y': -1.6}, {'image_id': '000306', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 2.22, 'bbox': [13.95, 169.57, 93.39, 206.59], 'dimensions': [1.7, 1.64, 2.85], 'location': [-26.79, 1.57, 34.79], 'rotation_y': 1.56}, {'image_id': '000306', 'type': 'Cyclist', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.82, 'bbox': [781.1, 178.08, 795.12, 209.15], 'dimensions': [1.64, 0.34, 1.58], 'location': [9.61, 1.93, 39.05], 'rotation_y': -1.58}, {'image_id': '000306', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [333.35, 171.7, 413.3, 189.66], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '000306', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [530.13, 162.88, 547.45, 184.97], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '000306', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [455.62, 173.13, 468.31, 181.86], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '000306', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [479.08, 174.89, 494.18, 179.0], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "\n",
      "[DEBUG] Applying transform: PILToTensor()[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/003812.png\n",
      "\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Original image size (PIL): (1242, 375)[DEBUG] get_label for 003812 returns 10 objects.Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/003886.png\n",
      "\n",
      "[DEBUG] Applying transform: Resize()\n",
      "\n",
      "[DEBUG] Original Image Size: (1242, 375)\n",
      "[DEBUG] get_label for 003886 returns 5 objects.[DEBUG] Applying transform: ToDtype()\n",
      "\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] Converting target for image_id 003886: [{'image_id': '003886', 'type': 'Car', 'truncated': 0.12, 'occluded': 0, 'alpha': -1.35, 'bbox': [1112.11, 174.83, 1241.0, 263.96], 'dimensions': [1.65, 1.67, 3.64], 'location': [12.15, 1.7, 15.22], 'rotation_y': -0.69}, {'image_id': '003886', 'type': 'Van', 'truncated': 0.18, 'occluded': 1, 'alpha': 3.04, 'bbox': [0.0, 149.33, 329.54, 317.02], 'dimensions': [2.01, 1.66, 4.35], 'location': [-6.55, 1.74, 10.32], 'rotation_y': 2.48}, {'image_id': '003886', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 2.27, 'bbox': [674.72, 178.13, 825.48, 249.4], 'dimensions': [1.59, 1.55, 3.93], 'location': [3.56, 1.75, 18.21], 'rotation_y': 2.46}, {'image_id': '003886', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 2.01, 'bbox': [914.48, 179.1, 1017.92, 225.5], 'dimensions': [1.47, 1.57, 4.24], 'location': [12.48, 1.71, 25.25], 'rotation_y': 2.47}, {'image_id': '003886', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.91, 'bbox': [1042.42, 178.15, 1124.04, 211.99], 'dimensions': [1.4, 1.68, 4.45], 'location': [21.31, 1.66, 32.48], 'rotation_y': 2.49}][DEBUG] Converting target for image_id 003812: [{'image_id': '003812', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.58, 'bbox': [558.88, 176.02, 604.3, 217.44], 'dimensions': [1.6, 1.76, 3.84], 'location': [-1.15, 1.24, 30.01], 'rotation_y': -1.62}, {'image_id': '003812', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.27, 'bbox': [204.48, 183.02, 449.67, 344.06], 'dimensions': [1.58, 1.68, 3.95], 'location': [-3.29, 1.57, 9.18], 'rotation_y': -1.6}, {'image_id': '003812', 'type': 'Car', 'truncated': 0.53, 'occluded': 0, 'alpha': -0.94, 'bbox': [0.0, 192.72, 185.44, 324.14], 'dimensions': [1.42, 1.53, 4.2], 'location': [-7.75, 1.57, 9.8], 'rotation_y': -1.6}, {'image_id': '003812', 'type': 'Van', 'truncated': 0.9, 'occluded': 1, 'alpha': -0.83, 'bbox': [0.0, 161.53, 52.27, 267.42], 'dimensions': [1.99, 1.89, 4.59], 'location': [-12.36, 1.56, 12.48], 'rotation_y': -1.6}, {'image_id': '003812', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.36, 'bbox': [380.8, 186.32, 476.78, 253.23], 'dimensions': [1.39, 1.5, 3.4], 'location': [-4.18, 1.44, 17.1], 'rotation_y': -1.6}, {'image_id': '003812', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.21, 'bbox': [228.19, 185.58, 348.9, 245.99], 'dimensions': [1.43, 1.62, 4.08], 'location': [-8.58, 1.47, 19.63], 'rotation_y': -1.61}, {'image_id': '003812', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.1, 'bbox': [140.6, 184.23, 254.75, 232.56], 'dimensions': [1.39, 1.5, 4.04], 'location': [-13.13, 1.38, 23.23], 'rotation_y': -1.61}, {'image_id': '003812', 'type': 'Van', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.4, 'bbox': [389.86, 157.61, 454.72, 216.42], 'dimensions': [2.58, 1.91, 6.53], 'location': [-8.89, 1.35, 34.98], 'rotation_y': -1.65}, {'image_id': '003812', 'type': 'Misc', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.51, 'bbox': [472.74, 152.0, 529.03, 213.77], 'dimensions': [2.64, 2.22, 5.52], 'location': [-4.93, 1.2, 33.75], 'rotation_y': -1.65}, {'image_id': '003812', 'type': 'Truck', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.57, 'bbox': [518.73, 151.46, 546.52, 195.82], 'dimensions': [4.05, 2.5, 16.44], 'location': [-7.71, 0.91, 74.12], 'rotation_y': -1.67}]\n",
      "\n",
      "[DEBUG] Original image size (PIL): (1242, 375)[DEBUG] Original image size (PIL): (1241, 376)[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "\n",
      "\n",
      "\n",
      "[DEBUG] Applying transform: Resize()Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/001448.png[DEBUG] Applying transform: Resize()\n",
      "\n",
      "\n",
      "[DEBUG] Original Image Size: (1242, 375)[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Original Image Size: (1241, 376)[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "\n",
      "[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] get_label for 001448 returns 5 objects.[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/003120.png[DEBUG] Converting target for image_id 001448: [{'image_id': '001448', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.4, 'bbox': [564.01, 169.92, 590.44, 188.58], 'dimensions': [1.48, 1.56, 3.62], 'location': [-1.93, 0.55, 59.2], 'rotation_y': -1.43}, {'image_id': '001448', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.57, 'bbox': [590.04, 169.39, 614.84, 192.32], 'dimensions': [1.5, 1.62, 3.88], 'location': [0.09, 0.7, 49.33], 'rotation_y': -1.57}, {'image_id': '001448', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.53, 'bbox': [538.19, 170.46, 567.27, 194.89], 'dimensions': [1.48, 1.7, 4.5], 'location': [-3.09, 0.81, 46.44], 'rotation_y': -1.61}, {'image_id': '001448', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 2.05, 'bbox': [329.66, 170.03, 398.24, 203.6], 'dimensions': [1.68, 1.67, 4.29], 'location': [-12.59, 1.1, 38.38], 'rotation_y': 1.73}, {'image_id': '001448', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [617.53, 168.45, 649.41, 191.31], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "\n",
      "[DEBUG] get_label for 003120 returns 20 objects.[DEBUG] Original image size (PIL): (1238, 374)\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] Applying transform: Resize()\n",
      "[DEBUG] Converting target for image_id 003120: [{'image_id': '003120', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.56, 'bbox': [594.54, 185.04, 658.57, 237.85], 'dimensions': [1.36, 1.69, 3.38], 'location': [0.45, 1.74, 20.86], 'rotation_y': -1.54}, {'image_id': '003120', 'type': 'Van', 'truncated': 1.0, 'occluded': 3, 'alpha': -2.46, 'bbox': [1052.67, 80.1, 1241.0, 374.0], 'dimensions': [2.12, 1.86, 4.41], 'location': [3.52, 1.71, 2.23], 'rotation_y': -1.51}, {'image_id': '003120', 'type': 'Car', 'truncated': 0.87, 'occluded': 3, 'alpha': -1.64, 'bbox': [0.0, 161.47, 123.58, 374.0], 'dimensions': [1.59, 1.63, 3.64], 'location': [-4.66, 1.56, 4.65], 'rotation_y': -2.4}, {'image_id': '003120', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.94, 'bbox': [43.05, 187.73, 344.66, 357.29], 'dimensions': [1.41, 1.64, 3.61], 'location': [-4.69, 1.59, 7.86], 'rotation_y': -2.47}, {'image_id': '003120', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -2.12, 'bbox': [191.7, 186.98, 445.16, 300.85], 'dimensions': [1.39, 1.61, 4.09], 'location': [-4.52, 1.62, 10.83], 'rotation_y': -2.51}, {'image_id': '003120', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -2.12, 'bbox': [386.69, 177.05, 516.53, 243.41], 'dimensions': [1.5, 1.57, 3.54], 'location': [-4.1, 1.63, 18.18], 'rotation_y': -2.34}, {'image_id': '003120', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -2.1, 'bbox': [410.01, 177.15, 517.21, 230.03], 'dimensions': [1.48, 1.56, 3.72], 'location': [-4.62, 1.64, 22.24], 'rotation_y': -2.3}, {'image_id': '003120', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -2.38, 'bbox': [478.39, 177.07, 563.72, 211.11], 'dimensions': [1.4, 1.6, 3.55], 'location': [-3.93, 1.6, 31.41], 'rotation_y': -2.5}, {'image_id': '003120', 'type': 'Van', 'truncated': 0.0, 'occluded': 3, 'alpha': -2.32, 'bbox': [549.12, 162.58, 600.89, 188.66], 'dimensions': [2.21, 1.82, 4.69], 'location': [-3.12, 1.37, 63.4], 'rotation_y': -2.37}, {'image_id': '003120', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.85, 'bbox': [762.73, 191.02, 922.45, 283.79], 'dimensions': [1.36, 1.63, 4.02], 'location': [3.97, 1.73, 13.14], 'rotation_y': -1.56}, {'image_id': '003120', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.77, 'bbox': [715.64, 173.95, 809.58, 240.11], 'dimensions': [1.66, 1.72, 4.41], 'location': [4.16, 1.72, 20.51], 'rotation_y': -1.57}, {'image_id': '003120', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.21, 'bbox': [699.6, 177.45, 721.93, 231.49], 'dimensions': [1.54, 0.84, 0.75], 'location': [2.82, 1.68, 21.12], 'rotation_y': -1.08}, {'image_id': '003120', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.69, 'bbox': [677.89, 172.39, 722.38, 207.7], 'dimensions': [1.72, 1.79, 3.94], 'location': [4.59, 1.73, 37.48], 'rotation_y': -1.57}, {'image_id': '003120', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.55, 'bbox': [604.85, 176.73, 642.46, 211.01], 'dimensions': [1.52, 1.67, 3.61], 'location': [0.64, 1.72, 34.05], 'rotation_y': -1.53}, {'image_id': '003120', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 2.13, 'bbox': [99.41, 172.05, 235.31, 222.3], 'dimensions': [1.47, 1.77, 4.25], 'location': [-14.27, 1.48, 23.51], 'rotation_y': 1.59}, {'image_id': '003120', 'type': 'Van', 'truncated': 0.0, 'occluded': 3, 'alpha': -0.31, 'bbox': [795.79, 163.95, 893.29, 202.37], 'dimensions': [2.51, 2.46, 5.85], 'location': [15.99, 1.96, 49.47], 'rotation_y': 0.0}, {'image_id': '003120', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [0.0, 170.18, 100.56, 373.0], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '003120', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [646.7, 163.07, 665.11, 183.42], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '003120', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [304.5, 168.89, 358.48, 184.06], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '003120', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [493.39, 159.18, 526.03, 174.35], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] Original Image Size: (1238, 374)[DEBUG] Original image size (PIL): (1242, 375)\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] Applying transform: Resize()\n",
      "\n",
      "[DEBUG] Applying transform: PILToTensor()[DEBUG] Batch image shapes: [torch.Size([3, 370, 1224]), torch.Size([3, 370, 1224]), torch.Size([3, 370, 1224]), torch.Size([3, 370, 1224])]\n",
      "[DEBUG] Target boxes shape: torch.Size([10, 4])\n",
      "[DEBUG] Target boxes shape: torch.Size([6, 4])\n",
      "[DEBUG] Target boxes shape: torch.Size([11, 4])\n",
      "[DEBUG] Target boxes shape: torch.Size([8, 4])\n",
      "Image sizes in this batch: [torch.Size([3, 370, 1224]), torch.Size([3, 370, 1224]), torch.Size([3, 370, 1224]), torch.Size([3, 370, 1224])]\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] Original Image Size: (1242, 375)\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] Applying transform: ToDtype()[DEBUG] Applying transform: PILToTensor()\n",
      "\n",
      "\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "\n",
      "\n",
      "\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])[DEBUG] Applying transform: ToDtype()[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/002146.png\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/003240.png[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/002978.png\n",
      "\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])[DEBUG] get_label for 003240 returns 12 objects.\n",
      "[DEBUG] get_label for 002146 returns 4 objects.\n",
      "[DEBUG] get_label for 002978 returns 14 objects.\n",
      "\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/000047.png[DEBUG] Converting target for image_id 003240: [{'image_id': '003240', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -3.03, 'bbox': [444.52, 177.4, 532.37, 208.17], 'dimensions': [1.47, 1.53, 4.21], 'location': [-6.12, 1.73, 36.45], 'rotation_y': 3.09}, {'image_id': '003240', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.68, 'bbox': [656.98, 175.37, 702.84, 213.77], 'dimensions': [1.64, 1.62, 4.2], 'location': [3.13, 1.78, 33.32], 'rotation_y': -1.59}, {'image_id': '003240', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': 1.68, 'bbox': [534.98, 181.95, 579.91, 217.5], 'dimensions': [1.35, 1.47, 3.43], 'location': [-2.14, 1.74, 29.66], 'rotation_y': 1.61}, {'image_id': '003240', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -2.93, 'bbox': [310.78, 176.63, 473.4, 232.06], 'dimensions': [1.48, 1.61, 4.24], 'location': [-6.23, 1.62, 20.84], 'rotation_y': 3.07}, {'image_id': '003240', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 0.43, 'bbox': [220.48, 177.12, 433.86, 256.91], 'dimensions': [1.51, 1.55, 3.72], 'location': [-5.7, 1.62, 14.65], 'rotation_y': 0.07}, {'image_id': '003240', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 0.42, 'bbox': [100.29, 176.71, 406.27, 282.99], 'dimensions': [1.58, 1.66, 4.21], 'location': [-5.74, 1.67, 11.88], 'rotation_y': -0.02}, {'image_id': '003240', 'type': 'Car', 'truncated': 0.27, 'occluded': 3, 'alpha': -2.53, 'bbox': [0.0, 173.21, 256.3, 306.47], 'dimensions': [1.55, 1.5, 3.4], 'location': [-6.66, 1.58, 9.27], 'rotation_y': -3.14}, {'image_id': '003240', 'type': 'Car', 'truncated': 0.63, 'occluded': 3, 'alpha': 0.71, 'bbox': [0.0, 137.68, 235.08, 371.77], 'dimensions': [1.87, 1.72, 4.1], 'location': [-6.01, 1.62, 6.65], 'rotation_y': -0.01}, {'image_id': '003240', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.76, 'bbox': [708.13, 190.78, 827.3, 267.33], 'dimensions': [1.27, 1.58, 4.03], 'location': [3.0, 1.68, 14.71], 'rotation_y': -1.56}, {'image_id': '003240', 'type': 'Car', 'truncated': 0.57, 'occluded': 0, 'alpha': -2.02, 'bbox': [799.53, 206.08, 1241.0, 374.0], 'dimensions': [1.32, 1.63, 4.1], 'location': [2.79, 1.66, 5.57], 'rotation_y': -1.58}, {'image_id': '003240', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [607.24, 165.65, 651.52, 191.82], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '003240', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [534.79, 168.24, 568.72, 176.94], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}][DEBUG] Converting target for image_id 002146: [{'image_id': '002146', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -2.98, 'bbox': [411.86, 185.38, 498.25, 214.77], 'dimensions': [1.47, 1.74, 4.26], 'location': [-8.23, 2.17, 38.44], 'rotation_y': 3.09}, {'image_id': '002146', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [534.79, 177.94, 656.7, 196.99], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '002146', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [864.05, 169.53, 877.28, 178.88], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '002146', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [879.57, 164.36, 912.85, 178.24], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}][DEBUG] Converting target for image_id 002978: [{'image_id': '002978', 'type': 'Car', 'truncated': 1.0, 'occluded': 0, 'alpha': -2.49, 'bbox': [1016.39, 176.35, 1241.0, 374.0], 'dimensions': [1.56, 1.57, 3.78], 'location': [2.91, 1.56, 1.93], 'rotation_y': -1.57}, {'image_id': '002978', 'type': 'Car', 'truncated': 0.03, 'occluded': 0, 'alpha': 2.05, 'bbox': [4.01, 176.39, 350.17, 374.0], 'dimensions': [1.73, 1.65, 3.7], 'location': [-4.42, 1.79, 8.02], 'rotation_y': 1.56}, {'image_id': '002978', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.84, 'bbox': [736.94, 177.7, 934.05, 308.24], 'dimensions': [1.54, 1.66, 3.79], 'location': [2.98, 1.63, 10.5], 'rotation_y': -1.57}, {'image_id': '002978', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.86, 'bbox': [294.4, 187.71, 448.27, 296.36], 'dimensions': [1.68, 1.63, 3.94], 'location': [-4.31, 1.99, 13.53], 'rotation_y': 1.56}, {'image_id': '002978', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.8, 'bbox': [693.17, 183.68, 809.63, 258.85], 'dimensions': [1.48, 1.64, 4.15], 'location': [3.05, 1.74, 16.61], 'rotation_y': -1.63}, {'image_id': '002978', 'type': 'Van', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.69, 'bbox': [654.52, 138.81, 748.97, 234.05], 'dimensions': [2.85, 2.46, 5.85], 'location': [2.92, 1.86, 24.7], 'rotation_y': -1.57}, {'image_id': '002978', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.66, 'bbox': [654.62, 176.38, 700.52, 220.27], 'dimensions': [1.78, 1.59, 4.33], 'location': [2.87, 1.96, 31.7], 'rotation_y': -1.57}, {'image_id': '002978', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 1.45, 'bbox': [648.04, 180.08, 693.7, 216.42], 'dimensions': [1.53, 1.57, 3.87], 'location': [2.67, 1.87, 32.72], 'rotation_y': 1.53}, {'image_id': '002978', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.66, 'bbox': [627.36, 185.76, 657.5, 210.17], 'dimensions': [1.45, 1.61, 3.34], 'location': [2.0, 2.29, 45.76], 'rotation_y': -1.61}, {'image_id': '002978', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.67, 'bbox': [506.0, 190.3, 536.85, 212.19], 'dimensions': [1.34, 1.69, 3.6], 'location': [-5.99, 2.54, 48.28], 'rotation_y': -1.79}, {'image_id': '002978', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.85, 'bbox': [604.79, 183.33, 640.47, 207.68], 'dimensions': [1.69, 1.59, 3.77], 'location': [0.86, 2.46, 52.69], 'rotation_y': -1.84}, {'image_id': '002978', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.78, 'bbox': [581.32, 187.22, 608.22, 204.8], 'dimensions': [1.49, 1.71, 3.58], 'location': [-1.41, 2.81, 64.95], 'rotation_y': -1.8}, {'image_id': '002978', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [543.2, 183.77, 579.72, 202.82], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '002978', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [483.04, 186.35, 502.09, 207.34], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "\n",
      "\n",
      "\n",
      "[DEBUG] Original image size (PIL): (1242, 375)[DEBUG] get_label for 000047 returns 9 objects.[DEBUG] Original image size (PIL): (1242, 375)[DEBUG] Original image size (PIL): (1242, 375)\n",
      "\n",
      "\n",
      "[DEBUG] Applying transform: Resize()[DEBUG] Applying transform: Resize()\n",
      "\n",
      "\n",
      "[DEBUG] Applying transform: Resize()[DEBUG] Converting target for image_id 000047: [{'image_id': '000047', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.57, 'bbox': [572.31, 171.6, 624.11, 218.82], 'dimensions': [1.6, 1.76, 3.84], 'location': [-0.38, 1.13, 26.54], 'rotation_y': -1.58}, {'image_id': '000047', 'type': 'Car', 'truncated': 0.43, 'occluded': 0, 'alpha': -0.92, 'bbox': [0.0, 190.67, 171.41, 313.27], 'dimensions': [1.47, 1.54, 3.51], 'location': [-8.37, 1.59, 10.68], 'rotation_y': -1.57}, {'image_id': '000047', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.22, 'bbox': [214.99, 165.2, 423.55, 292.98], 'dimensions': [1.78, 1.81, 4.32], 'location': [-4.65, 1.49, 12.25], 'rotation_y': -1.57}, {'image_id': '000047', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.24, 'bbox': [296.94, 182.19, 393.12, 231.61], 'dimensions': [1.45, 1.61, 4.3], 'location': [-8.54, 1.37, 23.68], 'rotation_y': -1.58}, {'image_id': '000047', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.38, 'bbox': [428.19, 180.81, 498.01, 226.24], 'dimensions': [1.45, 1.67, 3.88], 'location': [-5.0, 1.3, 25.13], 'rotation_y': -1.58}, {'image_id': '000047', 'type': 'Van', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.45, 'bbox': [481.87, 153.05, 537.09, 207.74], 'dimensions': [2.52, 2.02, 6.17], 'location': [-4.88, 1.02, 36.41], 'rotation_y': -1.58}, {'image_id': '000047', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.49, 'bbox': [522.27, 176.55, 552.71, 202.38], 'dimensions': [1.63, 1.67, 3.92], 'location': [-4.64, 1.07, 47.7], 'rotation_y': -1.58}, {'image_id': '000047', 'type': 'Car', 'truncated': 0.04, 'occluded': 1, 'alpha': -0.96, 'bbox': [0.0, 185.88, 180.39, 253.13], 'dimensions': [1.42, 1.57, 4.23], 'location': [-12.64, 1.46, 17.75], 'rotation_y': -1.57}, {'image_id': '000047', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.12, 'bbox': [193.59, 181.29, 294.71, 229.39], 'dimensions': [1.58, 1.64, 3.96], 'location': [-12.99, 1.45, 25.9], 'rotation_y': -1.58}]\n",
      "[DEBUG] Original Image Size: (1242, 375)[DEBUG] Original image size (PIL): (1241, 376)\n",
      "\n",
      "[DEBUG] Original Image Size: (1242, 375)\n",
      "\n",
      "[DEBUG] Applying transform: Resize()[DEBUG] Original Image Size: (1242, 375)\n",
      "\n",
      "[DEBUG] Original Image Size: (1241, 376)[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Applying transform: PILToTensor()\n",
      "\n",
      "[DEBUG] Applying transform: PILToTensor()[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Applying transform: ToDtype()\n",
      "\n",
      "\n",
      "\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Applying transform: ToDtype()[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Applying transform: PILToTensor()\n",
      "\n",
      "\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/000722.png\n",
      "\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/002733.png[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])[DEBUG] get_label for 000722 returns 9 objects.[DEBUG] get_label for 002733 returns 8 objects.\n",
      "\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/001756.png\n",
      "\n",
      "[DEBUG] Converting target for image_id 002733: [{'image_id': '002733', 'type': 'Truck', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.58, 'bbox': [516.29, 134.11, 583.11, 214.81], 'dimensions': [3.52, 2.89, 10.81], 'location': [-3.16, 1.88, 37.09], 'rotation_y': -1.66}, {'image_id': '002733', 'type': 'Car', 'truncated': 0.16, 'occluded': 1, 'alpha': 2.14, 'bbox': [0.0, 189.22, 72.94, 227.1], 'dimensions': [1.57, 1.47, 3.35], 'location': [-26.15, 2.34, 32.59], 'rotation_y': 1.47}, {'image_id': '002733', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 2.07, 'bbox': [62.23, 191.59, 130.56, 221.69], 'dimensions': [1.43, 1.48, 3.33], 'location': [-26.7, 2.44, 37.57], 'rotation_y': 1.45}, {'image_id': '002733', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 2.13, 'bbox': [125.46, 187.35, 195.2, 216.46], 'dimensions': [1.62, 1.6, 4.18], 'location': [-27.12, 2.52, 43.62], 'rotation_y': 1.57}, {'image_id': '002733', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 2.06, 'bbox': [183.23, 187.87, 233.32, 210.77], 'dimensions': [1.48, 1.35, 3.93], 'location': [-27.99, 2.56, 50.35], 'rotation_y': 1.55}, {'image_id': '002733', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [442.93, 167.59, 458.75, 184.7], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '002733', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [238.52, 180.53, 315.14, 204.76], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '002733', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [648.64, 172.12, 663.81, 183.41], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/003680.png\n",
      "\n",
      "[DEBUG] Converting target for image_id 000722: [{'image_id': '000722', 'type': 'Cyclist', 'truncated': 0.0, 'occluded': 3, 'alpha': -1.79, 'bbox': [817.4, 158.75, 855.83, 233.88], 'dimensions': [1.8, 0.52, 1.75], 'location': [5.65, 1.47, 18.18], 'rotation_y': -1.49}, {'image_id': '000722', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.89, 'bbox': [848.52, 164.33, 872.94, 220.88], 'dimensions': [1.7, 0.69, 0.68], 'location': [7.56, 1.45, 22.13], 'rotation_y': -1.57}, {'image_id': '000722', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.93, 'bbox': [874.29, 163.59, 895.92, 220.43], 'dimensions': [1.7, 0.48, 0.87], 'location': [8.28, 1.43, 22.05], 'rotation_y': -1.57}, {'image_id': '000722', 'type': 'Misc', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.88, 'bbox': [865.88, 186.74, 890.53, 219.55], 'dimensions': [1.0, 0.5, 0.77], 'location': [8.34, 1.44, 22.56], 'rotation_y': -1.53}, {'image_id': '000722', 'type': 'Cyclist', 'truncated': 0.0, 'occluded': 3, 'alpha': -1.66, 'bbox': [746.44, 170.25, 759.44, 205.54], 'dimensions': [1.64, 0.44, 1.56], 'location': [6.76, 1.53, 34.3], 'rotation_y': -1.47}, {'image_id': '000722', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [782.57, 155.27, 1240.0, 201.87], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '000722', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [671.58, 155.25, 800.08, 179.92], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '000722', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [577.27, 171.9, 635.64, 187.57], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '000722', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [481.44, 171.9, 498.15, 187.57], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "\n",
      "[DEBUG] Original image size (PIL): (1242, 375)[DEBUG] Original image size (PIL): (1242, 375)\n",
      "\n",
      "[DEBUG] get_label for 001756 returns 2 objects.[DEBUG] Applying transform: Resize()[DEBUG] get_label for 003680 returns 2 objects.[DEBUG] Applying transform: Resize()\n",
      "\n",
      "\n",
      "[DEBUG] Converting target for image_id 001756: [{'image_id': '001756', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': 1.77, 'bbox': [434.37, 185.3, 525.83, 246.66], 'dimensions': [1.5, 1.7, 4.21], 'location': [-3.55, 1.87, 20.23], 'rotation_y': 1.6}, {'image_id': '001756', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': 1.65, 'bbox': [550.02, 181.66, 579.29, 202.3], 'dimensions': [1.48, 1.87, 4.42], 'location': [-3.42, 2.16, 54.87], 'rotation_y': 1.59}]\n",
      "[DEBUG] Original Image Size: (1242, 375)\n",
      "[DEBUG] Converting target for image_id 003680: [{'image_id': '003680', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': 1.77, 'bbox': [646.07, 176.85, 672.01, 192.08], 'dimensions': [1.44, 1.68, 4.39], 'location': [4.84, 1.84, 70.84], 'rotation_y': 1.84}, {'image_id': '003680', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [712.03, 169.53, 719.44, 176.29], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "\n",
      "[DEBUG] Original image size (PIL): (1242, 375)[DEBUG] Original Image Size: (1242, 375)[DEBUG] Original image size (PIL): (1242, 375)\n",
      "\n",
      "[DEBUG] Applying transform: Resize()\n",
      "\n",
      "[DEBUG] Applying transform: Resize()[DEBUG] Original Image Size: (1242, 375)\n",
      "\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] Original Image Size: (1242, 375)\n",
      "\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/002338.png\n",
      "[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] Feature 0 (0.1.0.block): shape torch.Size([4, 16, 185, 612])\n",
      "[DEBUG] Feature 1 (0.2.1.block): shape torch.Size([4, 24, 93, 306])\n",
      "[DEBUG] Feature 2 (0.4.2.block): shape torch.Size([4, 80, 24, 77])\n",
      "[DEBUG] Feature 3 (0.6.3.block): shape torch.Size([4, 192, 12, 39])\n",
      "Type of x: <class 'list'>\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: PILToTensor()[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "\n",
      "[DEBUG] Applying transform: PILToTensor()[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] get_label for 002338 returns 9 objects.\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: ToDtype()\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/001277.png\n",
      "[DEBUG] Applying transform: ToDtype()[DEBUG] Converting target for image_id 002338: [{'image_id': '002338', 'type': 'Car', 'truncated': 0.06, 'occluded': 0, 'alpha': 1.08, 'bbox': [818.23, 177.84, 1215.42, 374.0], 'dimensions': [1.46, 1.61, 3.61], 'location': [3.38, 1.52, 6.92], 'rotation_y': 1.52}, {'image_id': '002338', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 2.06, 'bbox': [160.95, 192.65, 378.29, 301.54], 'dimensions': [1.48, 1.56, 4.09], 'location': [-5.61, 1.87, 12.41], 'rotation_y': 1.65}, {'image_id': '002338', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.87, 'bbox': [727.23, 177.86, 895.63, 277.91], 'dimensions': [1.46, 1.67, 3.89], 'location': [3.27, 1.57, 12.65], 'rotation_y': -1.63}, {'image_id': '002338', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.8, 'bbox': [425.9, 177.16, 491.16, 221.79], 'dimensions': [1.56, 1.63, 3.62], 'location': [-5.69, 1.74, 27.31], 'rotation_y': 1.59}, {'image_id': '002338', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.78, 'bbox': [678.41, 172.23, 758.85, 229.0], 'dimensions': [1.51, 1.6, 3.54], 'location': [3.06, 1.52, 21.19], 'rotation_y': -1.64}, {'image_id': '002338', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.79, 'bbox': [667.56, 172.82, 728.31, 217.38], 'dimensions': [1.53, 1.5, 3.34], 'location': [3.16, 1.56, 26.75], 'rotation_y': -1.68}, {'image_id': '002338', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': 1.73, 'bbox': [462.6, 177.23, 507.33, 211.93], 'dimensions': [1.46, 1.43, 3.52], 'location': [-5.59, 1.67, 32.37], 'rotation_y': 1.56}, {'image_id': '002338', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [801.3, 161.13, 838.47, 176.3], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '002338', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [0.0, 185.71, 90.86, 215.11], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "\n",
      "\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Original image size (PIL): (1242, 375)\n",
      "\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/001714.png[DEBUG] Applying transform: Resize()[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "\n",
      "Checking image path: /data/cmpe258-sp24/013978029/dataset/Kitti/raw/training/image_2/001350.png\n",
      "\n",
      "[DEBUG] Original Image Size: (1242, 375)[DEBUG] get_label for 001277 returns 9 objects.\n",
      "[DEBUG] Converting target for image_id 001277: [{'image_id': '001277', 'type': 'Car', 'truncated': 0.88, 'occluded': 0, 'alpha': -2.12, 'bbox': [823.26, 210.63, 1241.0, 374.0], 'dimensions': [1.53, 1.59, 3.64], 'location': [2.48, 1.82, 3.92], 'rotation_y': -1.59}, {'image_id': '001277', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': 1.73, 'bbox': [468.62, 185.15, 557.51, 259.69], 'dimensions': [1.63, 1.6, 3.73], 'location': [-2.35, 1.96, 18.0], 'rotation_y': 1.6}, {'image_id': '001277', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.72, 'bbox': [747.09, 204.33, 908.08, 343.23], 'dimensions': [1.45, 1.5, 3.26], 'location': [2.72, 1.94, 9.81], 'rotation_y': -1.46}, {'image_id': '001277', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': -1.74, 'bbox': [697.66, 192.59, 793.53, 276.13], 'dimensions': [1.59, 1.49, 3.34], 'location': [2.86, 2.07, 16.03], 'rotation_y': -1.57}, {'image_id': '001277', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.71, 'bbox': [666.11, 190.03, 738.29, 248.73], 'dimensions': [1.54, 1.6, 3.52], 'location': [2.62, 2.08, 21.44], 'rotation_y': -1.6}, {'image_id': '001277', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.74, 'bbox': [657.3, 189.73, 711.66, 233.52], 'dimensions': [1.55, 1.55, 3.27], 'location': [2.81, 2.24, 28.15], 'rotation_y': -1.64}, {'image_id': '001277', 'type': 'Car', 'truncated': 0.0, 'occluded': 2, 'alpha': -1.67, 'bbox': [645.37, 187.38, 688.72, 222.95], 'dimensions': [1.48, 1.6, 3.79], 'location': [2.53, 2.18, 33.06], 'rotation_y': -1.59}, {'image_id': '001277', 'type': 'Car', 'truncated': 0.0, 'occluded': 1, 'alpha': 1.64, 'bbox': [567.63, 179.58, 596.62, 209.04], 'dimensions': [1.91, 1.66, 4.22], 'location': [-1.88, 2.37, 49.18], 'rotation_y': 1.6}, {'image_id': '001277', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [598.83, 184.41, 639.88, 206.7], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "[DEBUG] Original image size (PIL): (1242, 375)\n",
      "[DEBUG] Applying transform: Resize()\n",
      "[DEBUG] Original Image Size: (1242, 375)\n",
      "[DEBUG] get_label for 001350 returns 4 objects.[DEBUG] get_label for 001714 returns 14 objects.\n",
      "\n",
      "\n",
      "[DEBUG] Converting target for image_id 001714: [{'image_id': '001714', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.24, 'bbox': [208.48, 181.03, 456.19, 329.94], 'dimensions': [1.5, 1.78, 3.69], 'location': [-3.25, 1.53, 9.18], 'rotation_y': -1.57}, {'image_id': '001714', 'type': 'Cyclist', 'truncated': 0.67, 'occluded': 1, 'alpha': -0.63, 'bbox': [1188.69, 129.05, 1223.0, 208.12], 'dimensions': [1.86, 0.63, 1.82], 'location': [15.15, 0.64, 16.92], 'rotation_y': 0.09}, {'image_id': '001714', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 0, 'alpha': 0.5, 'bbox': [596.98, 153.18, 643.29, 236.55], 'dimensions': [1.83, 0.69, 1.03], 'location': [0.31, 1.23, 16.07], 'rotation_y': 0.52}, {'image_id': '001714', 'type': 'Cyclist', 'truncated': 0.0, 'occluded': 1, 'alpha': -0.6, 'bbox': [895.6, 150.1, 946.02, 194.27], 'dimensions': [1.72, 0.6, 1.79], 'location': [12.53, 0.53, 28.09], 'rotation_y': -0.18}, {'image_id': '001714', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 2, 'alpha': 0.26, 'bbox': [394.4, 155.72, 428.66, 252.02], 'dimensions': [1.8, 0.61, 1.04], 'location': [-3.67, 1.34, 13.62], 'rotation_y': 0.0}, {'image_id': '001714', 'type': 'Cyclist', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.48, 'bbox': [896.04, 147.62, 926.12, 200.03], 'dimensions': [1.72, 0.78, 1.71], 'location': [10.46, 0.64, 24.19], 'rotation_y': -1.07}, {'image_id': '001714', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 1, 'alpha': 0.69, 'bbox': [149.42, 177.66, 182.55, 246.43], 'dimensions': [1.72, 0.55, 0.93], 'location': [-11.06, 1.66, 18.05], 'rotation_y': 0.15}, {'image_id': '001714', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 2, 'alpha': 0.63, 'bbox': [127.59, 181.34, 156.2, 248.35], 'dimensions': [1.62, 0.48, 0.96], 'location': [-11.09, 1.65, 17.46], 'rotation_y': 0.07}, {'image_id': '001714', 'type': 'Cyclist', 'truncated': 0.0, 'occluded': 2, 'alpha': -0.14, 'bbox': [298.4, 166.92, 391.34, 252.19], 'dimensions': [1.7, 0.64, 1.74], 'location': [-5.49, 1.43, 14.85], 'rotation_y': -0.48}, {'image_id': '001714', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 0, 'alpha': -2.6, 'bbox': [125.61, 178.15, 162.35, 248.88], 'dimensions': [1.6, 0.54, 0.84], 'location': [-10.67, 1.56, 16.44], 'rotation_y': 3.12}, {'image_id': '001714', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 0, 'alpha': -2.92, 'bbox': [67.17, 172.62, 117.87, 262.35], 'dimensions': [1.8, 0.54, 1.03], 'location': [-10.63, 1.65, 14.7], 'rotation_y': 2.75}, {'image_id': '001714', 'type': 'Pedestrian', 'truncated': 0.0, 'occluded': 2, 'alpha': -2.67, 'bbox': [233.14, 162.88, 259.99, 248.91], 'dimensions': [1.95, 0.56, 0.82], 'location': [-8.22, 1.55, 16.37], 'rotation_y': -3.12}, {'image_id': '001714', 'type': 'Car', 'truncated': 0.1, 'occluded': 1, 'alpha': -0.64, 'bbox': [1087.15, 149.7, 1223.0, 188.3], 'dimensions': [1.28, 1.7, 3.95], 'location': [19.7, 0.24, 25.14], 'rotation_y': 0.02}, {'image_id': '001714', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [460.73, 161.55, 531.77, 193.7], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}][DEBUG] Converting target for image_id 001350: [{'image_id': '001350', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.58, 'bbox': [583.52, 169.94, 605.61, 187.44], 'dimensions': [1.26, 1.6, 3.56], 'location': [-1.2, 1.07, 54.32], 'rotation_y': -1.6}, {'image_id': '001350', 'type': 'Car', 'truncated': 0.0, 'occluded': 0, 'alpha': -1.54, 'bbox': [557.93, 168.98, 577.32, 184.77], 'dimensions': [1.37, 1.63, 3.57], 'location': [-3.82, 1.06, 65.3], 'rotation_y': -1.6}, {'image_id': '001350', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [529.61, 160.48, 542.84, 169.18], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}, {'image_id': '001350', 'type': 'DontCare', 'truncated': -1.0, 'occluded': -1, 'alpha': -10.0, 'bbox': [599.15, 159.4, 616.9, 175.07], 'dimensions': [-1.0, -1.0, -1.0], 'location': [-1000.0, -1000.0, -1000.0], 'rotation_y': -10.0}]\n",
      "[DEBUG] Original image size (PIL): (1224, 370)\n",
      "[DEBUG] Applying transform: Resize()\n",
      "[DEBUG] Original Image Size: (1224, 370)\n",
      "\n",
      "[DEBUG] Original image size (PIL): (1242, 375)[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "\n",
      "[DEBUG] Applying transform: Resize()\n",
      "[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "\n",
      "[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] Original Image Size: (1242, 375)[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "\n",
      "[DEBUG] Applying transform: PILToTensor()[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Resized Image Tensor Shape: torch.Size([3, 370, 1224])\n",
      "\n",
      "[DEBUG] Applying transform: ToDtype()[DEBUG] After Resize(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>[DEBUG] Applying transform: PILToTensor()\n",
      "[DEBUG] After PILToTensor(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "[DEBUG] Applying transform: ToDtype()\n",
      "[DEBUG] After ToDtype(), image: <class 'torch.Tensor'>, target: <class 'dict'>\n",
      "\n",
      "[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])[DEBUG] Transformed image shape (Tensor): torch.Size([3, 370, 1224])\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (612) must match the size of tensor b (306) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Run the training/test process\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/cmpe258-sp24/013978029/DeepDataMiningLearning/DeepDataMiningLearning/detection/mytrain.py:160\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdistributed:\n\u001b[1;32m    159\u001b[0m     train_sampler\u001b[38;5;241m.\u001b[39mset_epoch(epoch)\n\u001b[0;32m--> 160\u001b[0m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39msaveeveryepoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m args\u001b[38;5;241m.\u001b[39mepochs:\n",
      "File \u001b[0;32m/data/cmpe258-sp24/013978029/DeepDataMiningLearning/DeepDataMiningLearning/detection/mytrain.py:231\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler, max_batches)\u001b[0m\n\u001b[1;32m    228\u001b[0m         loss_dict \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# Calculate total loss\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Reduce and accumulate loss for logging\u001b[39;00m\n\u001b[1;32m    234\u001b[0m loss_dict_reduced \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mreduce_dict(loss_dict)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (612) must match the size of tensor b (306) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/data/cmpe258-sp24/013978029/DeepDataMiningLearning') \n",
    "\n",
    "# Import necessary functions and classes from mytrain.py\n",
    "from DeepDataMiningLearning.detection.mytrain import get_args_parser, main\n",
    "\n",
    "# Simulate the argparse arguments\n",
    "class Args:\n",
    "    data_path = '/data/cmpe258-sp24/013978029/dataset/Kitti'  # Your dataset path\n",
    "    annotationfile = ''\n",
    "    dataset = 'kitti'\n",
    "    model = 'custom_fpn'\n",
    "    trainable = 0\n",
    "    device = 'cuda'  # Use 'cpu' if not running on a GPU\n",
    "    batch_size = 4  # Reduced batch size\n",
    "    epochs = 1  # Set to a small number for a test\n",
    "    saveeveryepoch = 1\n",
    "    workers = 4\n",
    "    opt = 'sgd'\n",
    "    lr = 0.02\n",
    "    momentum = 0.9\n",
    "    weight_decay = 1e-4\n",
    "    lr_scheduler = 'multisteplr'\n",
    "    lr_steps = [16, 22]\n",
    "    lr_gamma = 0.1\n",
    "    print_freq = 5\n",
    "    output_dir = './output'\n",
    "    resume = ''\n",
    "    start_epoch = 0\n",
    "    aspect_ratio_group_factor = -1\n",
    "    rpn_score_thresh = None\n",
    "    data_augmentation = 'hflip'\n",
    "    sync_bn = False\n",
    "    test_only = False\n",
    "    use_deterministic_algorithms = False\n",
    "    multigpu = False\n",
    "    world_size = 4\n",
    "    dist_url = 'env://'\n",
    "    weights = None\n",
    "    weights_backbone = None\n",
    "    amp = False\n",
    "    backend = 'PIL'\n",
    "    use_v2 = False\n",
    "    expname = 'experiment'\n",
    "    max_batches = 5  # Limit the number of batches for testing\n",
    "\n",
    "# Instantiate the arguments object\n",
    "args = Args()\n",
    "\n",
    "# Run the training/test process\n",
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmpe249project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
